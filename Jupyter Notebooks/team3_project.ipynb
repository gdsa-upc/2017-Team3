{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#importem la funcio get_params de params (funcio params)\n",
    "from params import get_params\n",
    "\n",
    "# definim la funcio build_database\n",
    "def build_database(params):\n",
    "\n",
    "    # List images\n",
    "    # llista les imatges dins del path root(GDSA)->database(TB2016) per als grups de train, val i test\n",
    "    image_names = os.listdir(os.path.join(params['root'],\n",
    "                             params['database'],params['split'],'images'))\n",
    "\n",
    "    # File to be saved\n",
    "    # crea el fitxer a la carpeta save dins de root, escriu la llista d'imatges\n",
    "    # i li dona el nom de split(train, val o test).txt\n",
    "    file = open(os.path.join(params['root'],params['root_save'],\n",
    "                             params['image_lists'],\n",
    "                             params['split'] + '.txt'),'w')\n",
    "\n",
    "    # Save image list to disk\n",
    "    # guarda el fitxer creat anteriorment i el tanca\n",
    "    for imname in image_names:\n",
    "        file.write(imname + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # crida a la funcio get_params dins de params.py\n",
    "    # obtenim tots els parametres necesaris per cridar la funcio build_database\n",
    "    params = get_params()\n",
    "\n",
    "    # cridem la funcio build_database  amb la que creem el fitxer de la llista d'imatges per cada grup\n",
    "    # train, val i test\n",
    "    for split in ['train','val','test']:\n",
    "        params['split'] = split\n",
    "        build_database(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking features together...\n",
      "Done. Time elapsed: 115.938817978\n",
      "Number of training features (429889, 128)\n",
      "Training codebook...\n",
      "Done. Time elapsed: 217.238002062\n",
      "Storing bow features for train set...\n",
      "Done. Time elapsed: 166.634321928\n",
      "Storing bow features for val set...\n",
      "Done. Time elapsed: 98.351831913\n",
      "Storing bow features for test set...\n",
      "Done. Time elapsed: 164.082720995\n"
     ]
    }
   ],
   "source": [
    "from params import get_params\n",
    "import sys\n",
    "\n",
    "# We need to add the source code path to the python path if we want to call modules such as 'utils'\n",
    "params = get_params()\n",
    "sys.path.insert(0,params['src'])\n",
    "\n",
    "from utils.rootsift import RootSIFT\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def get_features(params,pca=None,scaler=None):\n",
    "\n",
    "    # Read image names\n",
    "    readfile = os.path.join(params['root'],params['root_save'],\n",
    "                            params['image_lists'],params['split'] + '.txt')\n",
    "    with open(readfile,'r') as f:\n",
    "        image_list = f.readlines()\n",
    "\n",
    "    # Initialize keypoint detector and feature extractor\n",
    "    detector, extractor = init_detect_extract(params)\n",
    "\n",
    "    # Initialize feature dictionary\n",
    "    features = {}\n",
    "\n",
    "    # Get trained codebook\n",
    "    km = pickle.load(open(os.path.join(params['root'],params['root_save'],\n",
    "                                     params['codebooks_dir'],'codebook_'\n",
    "                                     + str(params['descriptor_size']) + \"_\"\n",
    "                                     + params['descriptor_type']\n",
    "                                     + \"_\" + params['keypoint_type'] + '.cb')\n",
    "                                     ,'rb'))\n",
    "\n",
    "    for image_name in image_list:\n",
    "\n",
    "        # Read image\n",
    "        im = cv2.imread(os.path.join(params['root'],params['database'],\n",
    "                                     params['split'],\n",
    "                                     'images',image_name.rstrip()))\n",
    "\n",
    "        # Resize image\n",
    "        im = resize_image(params,im)\n",
    "\n",
    "        # Extract local features\n",
    "        feats = image_local_features(im,detector,extractor)\n",
    "\n",
    "        if feats is not None:\n",
    "\n",
    "            if params['normalize_feats']:\n",
    "                feats = normalize(feats)\n",
    "\n",
    "            # If we scaled training features\n",
    "            if scaler is not None:\n",
    "                scaler.transform(feats)\n",
    "\n",
    "            # Whiten if needed\n",
    "            if pca is not None:\n",
    "\n",
    "                pca.transform(feats)\n",
    "\n",
    "            # Compute assignemnts\n",
    "            assignments = get_assignments(km,feats)\n",
    "\n",
    "            # Generate bow vector\n",
    "            feats = bow(assignments,km)\n",
    "        else:\n",
    "            # Empty features\n",
    "            feats = np.zeros(params['descriptor_size'])\n",
    "\n",
    "        # Add entry to dictionary\n",
    "        features[image_name] = feats\n",
    "\n",
    "\n",
    "    # Save dictionary to disk with unique name\n",
    "    save_file = os.path.join(params['root'],params['root_save'],\n",
    "                             params['feats_dir'],\n",
    "                             params['split'] + \"_\" +\n",
    "                             str(params['descriptor_size']) + \"_\"\n",
    "                             + params['descriptor_type'] + \"_\"\n",
    "                             + params['keypoint_type'] + '.p')\n",
    "\n",
    "    pickle.dump(features,open(save_file,'wb'))\n",
    "\n",
    "\n",
    "def resize_image(params,im):\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width = im.shape[:2]\n",
    "\n",
    "    # If the image width is smaller than the proposed small dimension,\n",
    "    # keep the original size !\n",
    "    resize_dim = min(params['max_size'],width)\n",
    "\n",
    "    # We don't want to lose aspect ratio:\n",
    "    dim = (resize_dim, height * resize_dim/width)\n",
    "\n",
    "    # Resize and return new image\n",
    "    return cv2.resize(im,dim)\n",
    "\n",
    "def image_local_features(im,detector,extractor):\n",
    "\n",
    "    '''\n",
    "    Extract local features for given image\n",
    "    '''\n",
    "\n",
    "    positions = detector.detect(im,None)\n",
    "    positions, descriptors = extractor.compute(im,positions)\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "def init_detect_extract(params):\n",
    "\n",
    "    '''\n",
    "    Initialize detector and extractor from parameters\n",
    "    '''\n",
    "    if params['descriptor_type'] == 'RootSIFT':\n",
    "\n",
    "        extractor = RootSIFT()\n",
    "    else:\n",
    "        extractor = cv2.xfeatures2d.SIFT_create()\n",
    "        #extractor = cv2.DescriptorExtractor_create(params['descriptor_type'])\n",
    "\n",
    "    #detector = cv2.FeatureDetector_create(params['keypoint_type'])\n",
    "    detector = cv2.xfeatures2d.SIFT_create()\n",
    "    return detector, extractor\n",
    "\n",
    "def stack_features(params):\n",
    "\n",
    "    '''\n",
    "    Get local features for all training images together\n",
    "    '''\n",
    "\n",
    "    # Init detector and extractor\n",
    "    detector, extractor = init_detect_extract(params)\n",
    "\n",
    "    # Read image names\n",
    "    readfile = os.path.join(params['root'],params['root_save'],\n",
    "                            params['image_lists'],params['split'] + '.txt')\n",
    "\n",
    "    with open(readfile,'r') as f:\n",
    "        image_list = f.readlines()\n",
    "\n",
    "    X = []\n",
    "    for image_name in image_list:\n",
    "\n",
    "        # Read image\n",
    "        im = cv2.imread(os.path.join(params['root'],\n",
    "                        params['database'],params['split'],\n",
    "                        'images',image_name.rstrip()))\n",
    "\n",
    "        # Resize image\n",
    "        im = resize_image(params,im)\n",
    "\n",
    "        feats = image_local_features(im,detector,extractor)\n",
    "        # Stack all local descriptors together\n",
    "\n",
    "        if feats is not None:\n",
    "            if len(X) == 0:\n",
    "\n",
    "                X = feats\n",
    "            else:\n",
    "                X = np.vstack((X,feats))\n",
    "\n",
    "    if params['normalize_feats']:\n",
    "        X = normalize(X)\n",
    "\n",
    "    if params['whiten']:\n",
    "\n",
    "        pca = PCA(whiten=True)\n",
    "        pca.fit_transform(X)\n",
    "\n",
    "    else:\n",
    "        pca = None\n",
    "\n",
    "    # Scale data to 0 mean and unit variance\n",
    "    if params['scale']:\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        scaler.fit_transform(X)\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    return X, pca, scaler\n",
    "\n",
    "def train_codebook(params,X):\n",
    "\n",
    "    # Init kmeans instance\n",
    "    km = MiniBatchKMeans(params['descriptor_size'])\n",
    "\n",
    "    # Training the model with our descriptors\n",
    "    km.fit(X)\n",
    "\n",
    "    # Save to disk\n",
    "    pickle.dump(km,open(os.path.join(params['root'],params['root_save'],\n",
    "                                     params['codebooks_dir'],'codebook_'\n",
    "                                     + str(params['descriptor_size']) + \"_\"\n",
    "                                     + params['descriptor_type']\n",
    "                                     + \"_\" + params['keypoint_type'] + '.cb'),\n",
    "                                     'wb'))\n",
    "\n",
    "    return km\n",
    "\n",
    "def get_assignments(km,descriptors):\n",
    "\n",
    "    assignments = km.predict(descriptors)\n",
    "\n",
    "    return assignments\n",
    "\n",
    "\n",
    "def bow(assignments,km):\n",
    "\n",
    "    # Initialize empty descriptor of the same length as the number of clusters\n",
    "    descriptor = np.zeros(np.shape(km.cluster_centers_)[0])\n",
    "\n",
    "    # Build vector of repetitions\n",
    "    for a in assignments:\n",
    "\n",
    "        descriptor[a] += 1\n",
    "\n",
    "    # L2 normalize\n",
    "    descriptor = descriptor.reshape(1, -1)\n",
    "    descriptor = normalize(descriptor)\n",
    "\n",
    "    return descriptor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    params = get_params()\n",
    "\n",
    "    # Change to training set\n",
    "    params['split'] = 'train'\n",
    "\n",
    "    print \"Stacking features together...\"\n",
    "    # Save features for training set\n",
    "    t = time.time()\n",
    "    X, pca, scaler = stack_features(params)\n",
    "    print \"Done. Time elapsed:\", time.time() - t\n",
    "    print \"Number of training features\", np.shape(X)\n",
    "\n",
    "    print \"Training codebook...\"\n",
    "    t = time.time()\n",
    "    train_codebook(params,X)\n",
    "    print \"Done. Time elapsed:\", time.time() - t\n",
    "\n",
    "    for split in ['train','val','test']:\n",
    "        params['split'] = split\n",
    "        print \"Storing bow features for %s set...\"%(params['split'])\n",
    "        t = time.time()\n",
    "        get_features(params, pca,scaler)\n",
    "        print \"Done. Time elapsed:\", time.time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/home/aleix/Documentos/Q5/GDSA/save/rankings/SIFT/test/wgsoteylpq.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-758796545e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-758796545e1c>\u001b[0m in \u001b[0;36mrank\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     47\u001b[0m             outfile = open(os.path.join(params['root'],params['root_save'],\n\u001b[1;32m     48\u001b[0m                            \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rankings_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'descriptor_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                            split,id.split('.')[0] + '.txt'),'w')\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/home/aleix/Documentos/Q5/GDSA/save/rankings/SIFT/test/wgsoteylpq.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from params import get_params\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def rank(params):\n",
    "\n",
    "    train_features = pickle.load(open(os.path.join(params['root'],\n",
    "                                 params['root_save'],params['feats_dir'],\n",
    "                                 'train' + \"_\" + str(params['descriptor_size'])\n",
    "                                 + \"_\" + params['descriptor_type'] +\n",
    "                                 \"_\" + params['keypoint_type'] + '.p'),'rb'))\n",
    "\n",
    "    for split in ['val','test']:\n",
    "        features = pickle.load(open(os.path.join(params['root'],\n",
    "                                   params['root_save'],params['feats_dir'],\n",
    "                                   split + \"_\" +\n",
    "                                   str(params['descriptor_size']) + \"_\"\n",
    "                                   + params['descriptor_type'] + \"_\"\n",
    "                                   + params['keypoint_type'] + '.p'),'rb'))\n",
    "\n",
    "\n",
    "        # For each image id in the validation set\n",
    "        for id in features.keys():\n",
    "\n",
    "            # Get its feature\n",
    "            bow_feats = features[id]\n",
    "\n",
    "            # The ranking is composed with the ids of all training images\n",
    "            ranking = train_features.keys()\n",
    "\n",
    "            X = np.array(train_features.values())\n",
    "\n",
    "            # The .squeeze() method reduces the dimensions of an array to the\n",
    "            # minimum. E.g. if we have a numpy array of shape (400,1,100)\n",
    "            # it will transform it to (400,100)\n",
    "            distances = pairwise_distances(bow_feats,X.squeeze())\n",
    "\n",
    "\n",
    "            # Sort the ranking according to the distances.\n",
    "            # We convert 'ranking' to numpy.array to sort it, and then back to list\n",
    "            # (although we could leave it as numpy array).\n",
    "            ranking = list(np.array(ranking)[np.argsort(distances.squeeze())])\n",
    "\n",
    "            # Save to text file\n",
    "            outfile = open(os.path.join(params['root'],params['root_save'],\n",
    "                           params['rankings_dir'],params['descriptor_type'],\n",
    "                           split,id.split('.')[0] + '.txt'),'w')\n",
    "\n",
    "            for item in ranking:\n",
    "                outfile.write(item.split('.')[0] + '\\n')\n",
    "            outfile.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    params = get_params()\n",
    "    rank(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ens dona un error que no sabem a què es degut."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
